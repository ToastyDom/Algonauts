<!doctype html>
<html ng-app="momentsApp" lang="en">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VCQ9D4SNB4"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-VCQ9D4SNB4');
    </script>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Challenge &ndash; Algonauts Project 2025</title>
    <meta name="description" content="Challenge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Alex Lascelles">
    <link rel="icon" id="favicon" type="image/png" href="img/2024/logo_icon_light.png" sizes="32x32" />
    <link rel="stylesheet" href="css/normalize.min.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/jquery.fancybox.css">
    <link rel="stylesheet" href="css/flexslider.css">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/angulargrid.css">
    <link rel="stylesheet" href="css/queries.css">
    <link rel="stylesheet" href="css/etline-font.css">
    <link rel="stylesheet" href="node_modules/angular-material/angular-material.min.css">
    <link rel="stylesheet" href="bower_components/animate.css/animate.min.css">
    <link href="https://use.fontawesome.com/releases/v5.0.3/css/all.css" rel="stylesheet">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <script async src="js/vendor/modernizr-2.8.3-respond-1.4.2.min.js"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes:
        true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS
        to left justify single line equations in code cells. displayAlign: 'center', "HTML-CSS": { styles: {'.MathJax_Display':
        {"margin": 0}}, linebreaks: { automatic: true } } });
    </script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const favicon = document.getElementById('favicon');
            const updateFavicon = () => {
                if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
                    favicon.href = 'img/2024/logo_icon_dark.png';
                } else {
                    favicon.href = 'img/2024/logo_icon_light.png';
                }
            };

            // Initial check
            updateFavicon();

            // Listen for changes in the color scheme
            window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', updateFavicon);
        });
    </script>
</head>

<body id="top" ng-controller="mainCtrl">
    <section class="hero-video-container" style="position: relative;">
        <div class="video-wrapper">
            <video class="full-screen-video" src="img/2024/movie_banner_mixed2.mp4" autoplay loop muted></video>
            <div class="video-overlay">
                <h1 style="color: white; font-weight: bold; margin-bottom: 0;">The 2025 Challenge:</h1>
                <h1 style="font-size: 29px; margin-bottom: 25px; color: white; font-style: italic;">
                    <b>How the Human Brain Makes Sense of Multimodal Movies</b>
                </h1>
            </div>
        </div>
    </section>
    
    
    <body class="with-nav-text">
        <section class="navigation">
            <header>
                <div class="header-content">
                    <div class="nav-brand">
                        <a href="index.html">
                            <img src="img/2024/logo_icon_dark.png" alt="Algonauts Logo" class="nav-logo-img">
                        </a>
                        <div class="nav-text">
                            <a href="index.html">The Algonauts Project</a>
                        </div>
                    </div>     
                    <div class="header-nav navbar-right">
                        <nav>
                            <ul class="primary-nav">
                                <li><a href="index.html#about">About</a></li>
                                <li><a href="challenge.html">Challenge</a></li>
                                <li><a href="braindata.html">Brain Data</a></li>
                                <li><a href="encoding.html">Encoding Models</a></li>
                                <li><a href="index.html#cite">Papers</a></li>
                                <li><a href="index.html#team">Team</a></li>
                                <li><a href="index.html#contact">Contact</a></li>
                                <li><a href="archive.html">Archive</a></li>
                            </ul>
                        </nav>
                    </div>
                    <div class="nav-overlay">
                        <nav class="overlay-nav">
                            <ul>
                                <li><a href="index.html#about">About</a></li>
                                <li><a href="challenge.html">Challenge</a></li>
                                <li><a href="braindata.html">Brain Data</a></li>
                                <li><a href="encoding.html">Encoding Models</a></li>
                                <li><a href="index.html#cite">Papers</a></li>
                                <li><a href="index.html#team">Team</a></li>
                                <li><a href="index.html#contact">Contact</a></li>
                                <li><a href="archive.html">Archive</a></li>
                            </ul>
                        </nav>
                    </div>
                    <div class="navicon">
                        <a class="nav-toggle" href="#">
                            <span></span>
                        </a>
                    </div>
                </div>
            </header>
        </section>
    </body>

        <section class="challenge intro-section-first-padding" id="challenge_about">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h2 style="margin-bottom: 1em" id="challenge_overview">Challenge Overview</h2>
                    <div style="vertical-align:middle; text-align:center">
                        <video src="img/2024/2025_challenge_overview.mp4" style="height: auto; width: auto; max-width: 100%;" autoplay loop muted playsinline>
                            Your browser does not support the video tag.
                        </video>
                    </div>    
                    <p class="intro-paragraph" style="text-align: justify; margin-top: 1em">
                        Understanding how the human brain works is one of the key challenges that science and society face. The Algonauts 
                        challenge proposes a test of how well computational models do today. This test is intrinsically <b style="font-weight: 600">open</b> and
                        <b style="font-weight: 600">quantitative</b>. This will allow us to precisely assess the progress in explaining the human brain. 
                    </p>
                    <p class="intro-paragraph" style="text-align: justify;">
                        At every instant, we are flooded by a massive amount of visual and auditory (sound and language) information - 
                        and yet, we perceive the world as ordered and meaningful actions, events and language. The primary 
                        target of the 2025 Challenge is predicting human brain responses to complex naturalistic multimodal 
                        movies, using the largest available brain dataset for this purpose.
                    </p>

                    <p class="intro-paragraph" style="text-align: justify;">
                        Watching multimodal movies activates large swathes of the human cortex. We pose the question: 
                        How well does your computational model account for these activations?
                    </p>


                    <div class="col-md-6 col-xs-12" style="margin-bottom: 2em">
                        <div class="responsive-youtube">
                            <iframe src="https://www.youtube.com/embed/KlwSDpxUX6k?vq=hd1080&rel=0" width="560" height="315" title="The Algonauts Project 2023: Challenge & development kit tutorial walk-through" frameborder="0" allowfullscreen></iframe>
                        </div>
                    </div>

                    <div class="col-md-6 col-xs-12" style="margin-bottom: 2em">
                        <div class="responsive-youtube">
                            <iframe src="https://www.youtube.com/embed/6b8OuMSXIpA?vq=hd1080&rel=0" width="560" height="315" title="The Algonauts Project 2023: CodaLab Submission Walkthrough" frameborder="0" allowfullscreen></iframe>
                        </div>
                    </div>

                    <p class="intro-paragraph" style="text-align: justify;">
                        Watch the first video above for an introduction to the Algonauts 2025 challenge, and the second video for a detailed walkthrough 
                        of the <a href="https://colab.research.google.com/drive/1fop0zvaLBLBagvJRC-HDqGDSgQElNWZB?usp=sharing" class="a-2021" target="_blank">development kit</a>. 
                        When you are ready to participate, the third video will guide you through the <a href="https://www.codabench.org/competitions/4313/" class="a-2021" target="_blank">Codabench</a> competition submission process.
                    </p>

                </div>
            </div>
        </div>
    </section>  

    <section class="challenge intro-section-padding" id="Competition">
        <div class="container">
            <div class="row wow fadeInUp">
                <div class="col-md-12">
                    <div class="challenge-icon">
                        <span class="icon filter-purple">
                            <img src="img/trophy3.svg" class="icon" style="height:55px;">
                        </span>
                    </div>
                    <h2 style="margin-bottom: 1em">Competition</h2>

                    <p class="intro-paragraph" style="text-align: justify;">
                        The goal of the 2025 challenge is to provide a platform for biological and artificial intelligence scientists to cooperate 
                        and compete in developing cutting-edge brain encoding models. Specifically, these models should predict whole brain
                        response to multimodal naturalistic stimulation, and generalize outside their training distribution.
                    </p>
                    <p class="intro-paragraph" style="text-align: justify;">
                        The Challenge data is based on the <a href="https://www.cneuromod.ca/" class="a-2021" target="_blank">CNeuroMod dataset</a> which, as of now, most intensively samples single subject 
                        neural responses to a variety of controlled and naturalistic stimuli and tasks (over 160 hours per subject). 
                        The CNeuroMod dataset’s unprecedented size, combined with the multimodal nature and diversity of its stimuli 
                        and tasks, makes it an ideal training and testing ground to build robust encoding models of fMRI responses 
                        to multimodal stimuli that generalize outside of their training distribution. <a href="braindata.html" class="a-2021" target="_blank">Learn more about the stimuli and fMRI dataset used in the 2025 Challenge.</a>
                    </p>
                    <p class="intro-paragraph" style="text-align: justify; display: block; margin-bottom: 0;">
                        We provide:
                    </p>
                    <ul class="intro-paragraph" style="list-style-type: disc; margin-left: 20px;">
                        <li>a set of movie stimuli, and</li>
                        <li>the corresponding brain responses recorded while four human subjects viewed those movies.</li>
                    </ul>
                    
                    
                    <p class="intro-paragraph" style="text-align: justify;">
                        <br>With that, challenge participants are expected to build computational models to predict brain responses to in-distribution (ID) and out-of-distribution (OOD)  movies for which the brain data are withheld.
                    </p>
                    <p class="intro-paragraph" style="text-align: justify;">
                        Challenge participants submit <b style="font-weight: 600">predicted responses</b>  in the format described in the 
                        <a href="https://colab.research.google.com/drive/1fop0zvaLBLBagvJRC-HDqGDSgQElNWZB?usp=sharing" class="a-2021" target="_blank">development kit</a>. 
                        We score the submission by measuring the predictivity for each brain parcel for all 
                        the subjects and display on the leaderboard the overall mean predictivity  over all parcels and subjects.
                    </p>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-md-12 text-center">
                                <a style="margin: 1em; font-weight: bold; font-size:14px;" href="https://forms.gle/kmgYdxR92H4nUBfH7" class="btn-2021 btn-accent btn-large">Download the Challenge Data</a>
                                <a style="margin: 1em; font-weight: bold; font-size:14px;" href="https://www.codabench.org/competitions/4313/" class="btn-2021 btn-accent btn-large">Participate in the 2025 challenge</a>
                            </div>
                        </div>
                    </div>                    
                </div>
            </div>
        </div>
    </section>



    <section class="challenge intro-section-padding" id="ChallengeData">
        <div class="container">
            <div class="row wow fadeInUp">
                <div class="col-md-12">
                    <h2 style="margin-bottom: 1em">Challenge Phases</h2>
                    <p class="intro-paragraph" style="text-align: justify;">
                        The challenge is hosted on Codabench, and consists of two main serial phases:
                    </p>
                    <ul class="intro-paragraph" style="list-style-type: disc; margin-left: 20px;">
                        <li>
                            a <b style="font-weight: 600">6-months model building phase</b> during which encoding models are trained and tested, followed by
                        </li>
                        <li>
                            a <b style="font-weight: 600">1-week model selection phase</b> during which the winning models are selected based on their performance on a withheld <b style="font-weight: 600">out-of-distribution (OOD) test set</b>.
                        </li>
                    </ul>
                    <p class="intro-paragraph" style="text-align: justify; margin-top: 1em;">
                        To enforce strict tests of OOD generalization during the model selection phase, 
                        the two phases are based on data from different distributions, and have separate leaderboards. 
                        The challenge will be followed by an <b style="font-weight: 600">indefinite post-challenge phase</b>, which will serve as a
                        public benchmark for anyone wishing to test brain encoding models on multimodal movie data (<b style="font-weight: 600">Figure 1</b>).
                    </p>
                </div>
            </div>
            <div class="row justify-content-center text-center" style="margin-top: 2em;">
                <div class="large-image-container" style="text-align: center;">
                    <img src="img/2024/figure_2_panel_c.png" alt="Figure 1" class="large-image">
                    <p style="font-style: italic; color: #666; font-size: 1.4em; margin-top: 10px;">
                        <b style="font-weight: 600;">Figure 1 | Challenge phases.</b> 
                        During the model building phase, models are trained using stimuli and corresponding fMRI responses for 
                        seasons 1 to 6 of the sitcom Friends and Movie10 (a set of four movies), and tested in-distribution (ID) 
                        on Friends season 7 (for which the fMRI responses are withheld) with unlimited submissions. 
                        During the model selection phase, the winning models are selected based on the accuracy of their 
                        predicted fMRI responses for out-of-distribution (OOD) movie stimuli (for which the fMRI responses 
                        are withheld) with up to ten submissions. The challenge will be 
                        followed by an indefinite post-challenge phase with unlimited submissions, 
                        which will serve as a public benchmark for both ID and OOD model validation.
                    </p>
                </div>
            </div>
            
            
     
                    
                    
    
                    

                    <h3>Model Building Phase (6 months, January 6 2025 - July 6 2025)</h3>
                    <p class="intro-paragraph" style="text-align: justify;">
                        During this first phase, challenge participants will train and test encoding models using movie stimuli and fMRI responses from the same distribution.
                    </p>
                    <ul>
                        <li style="margin-bottom: 0.5em">
                            <b style="font-weight: 600">Model Training:</b> For model training, we provide 65 hours of movie stimuli and 
                            corresponding fMRI responses: 55 hours for each of the four subjects for all episodes of seasons 1 to 6 of the sitcom <i>Friends</i>, 
                            and 10 hours for the following  four movies: <i>The Bourne Supremacy</i>, <i>Hidden Figures</i>, <i>Life</i> (a BBC nature documentary), 
                            and <i>The Wolf of Wall Street</i>. Each movie was presented to each of the 4 subjects once, 
                            except for <i>Life</i> and <i>Hidden Figures</i> which were presented twice. Challenge participants can 
                            train their encoding models using these data.
                        <li>
                            <b style="font-weight: 600">Model Testing:</b> For model testing, we provide 10 hours of movie stimuli for all episodes of seasons 
                            7 of the Friends dataset, and withhold the corresponding fMRI responses for each subject. Challenge participants 
                            can test their encoding models against the withheld fMRI responses by submitting predicted fMRI responses for 
                            Friends season 7 to <a href="https://www.codabench.org/competitions/4313/" class="a-2021" target="_blank">Codabench</a>. After each submission, the scoring program will correlate (Pearson’s <i>r</i>) 
                            the predicted fMRI responses for each parcel and subject with the recorded (withheld) fMRI responses across all 
                            Friends season 7 episodes, resulting in one correlation score for each parcel and subject. These correlation scores 
                            are averaged first across parcels and then across subjects, to obtain a single correlation score quantifying the performance of each submission.
                        </li>                            
                    </ul>

                    <div class="text-center" style="margin-top: 3em;">
                        <a href="https://www.codabench.org/competitions/4313/#/results-tab" 
                           class="btn-2021 btn-accent btn-large" 
                           style="padding: 0.8em 1.5em; font-size: 14px; font-weight: bold;" 
                           target="_blank">MODEL BUILDING PHASE LEADERBOARD</a>
                    </div>
                    

                    <h3 style="margin-top: 2em">Model Selection Phase (1 week, July 6 2025 - July 13 2025)</h3>
                    <p class="intro-paragraph" style="text-align: justify;">
                        During this second phase, the winning models will be selected based on the accuracy of their predicted fMRI responses for withheld OOD movie stimuli.
                    </p>

                    <ul>
                        <li style="margin-bottom: 0.5em">
                            <b style="font-weight: 600">Model Testing:</b> At the beginning of the model selection phase, we will provide 2 hours of 
                            OOD movie stimuli and withhold the corresponding fMRI responses for each of the four subjects. The nature of the OOD movie 
                            stimuli will not be revealed until the beginning of the model selection phase. To participate in the winners selection process, 
                            challenge participants can submit their encoding model’s predicted fMRI responses for the OOD movie stimuli to <a href="https://www.codabench.org/competitions/4313/" class="a-2021" target="_blank">Codabench</a>. 
                            After each submission, the scoring program will correlate the predicted fMRI responses for each parcel and subject with the 
                            recorded (withheld) fMRI responses, independently for each of the OOD movie stimuli, resulting in one correlation score for 
                            each parcel, OOD movie and subject. These correlation scores are averaged first across parcels, then across OOD movies, and 
                            finally across subjects, thus obtaining a single correlation score quantifying the performance of each submission.
                        </li>
                    </ul>

                    <div class="text-center" style="margin-top: 3em;">
                        <a href="https://www.codabench.org/competitions/4313/#/results-tab" 
                           class="btn-2021 btn-accent btn-large" 
                           style="padding: 0.8em 1.5em; font-size: 14px; font-weight: bold;" 
                           target="_blank">MODEL SELECTION PHASE LEADERBOARD</a>
                    </div>

                    <h3 style="margin-top: 2em">Post-Challenge Phase (indefinite)</h3>
                    <p class="intro-paragraph" style="text-align: justify;">
                        Once the challenge is over, we will open an indefinite post-challenge phase which will serve as a public benchmark. 
                        This benchmark will consist of two separate leaderboards that will rank encoding models based on their fMRI predictions 
                        for ID (Friends season 7) or OOD (OOD movies) multimodal movie stimuli, respectively.
                    </p>

                    <div class="text-center" style="margin-top: 3em;">
                        <a href="https://www.codabench.org/competitions/4313/#/results-tab" 
                           class="btn-2021 btn-accent btn-large" 
                           style="padding: 0.8em 1.5em; font-size: 14px; font-weight: bold;" 
                           target="_blank">POST-CHALLENGE PHASE LEADERBOARD</a>
                    </div>

                    <h3 style="margin-top: 2em">Development Kit</h3>
                    <p class="intro-paragraph" style="text-align: justify;">
                        To facilitate participation, we provide a 
                        <a href="https://colab.research.google.com/drive/1fop0zvaLBLBagvJRC-HDqGDSgQElNWZB?usp=sharing" class="a-2021" target="_blank">development kit</a> in
                         Python which accompanies users through the challenge process, following four steps:
                        </p>
                        <ol class="intro-paragraph" style="text-align: justify; padding-left: 80px;">
                            <li>Familiarizing with the challenge data.</li>
                            <li>Extracting the stimulus features used to train and validate an fMRI encoding model.</li>
                            <li>Training and validating an fMRI encoding model.</li>
                            <li>Preparing the predicted fMRI responses for the test stimuli in the correct format for submission to Codabench.</li>
                        </ol>


                    <h3 style="margin-top: 1em">How to Predict Brain Data Using Computational Models?</h3>
                    <p class="intro-paragraph" style="text-align: justify;">
                        There are different ways to predict brain data using computational models. We put close to no restrictions on how you do so (see <a href="https://algonautsproject.com/challenge.html#Rules" class="a-2021">Challenge Rules</a>). 
                        However, a commonly used approach is to use linearizing encoding models, and we provide a development kit to implement such a model.</p>
                    <p class="intro-paragraph" style="text-align: justify;">
                        <a href="encoding.html" class="a-2021">Click here to learn more about linearizing encoding.</a>
                    </p>
                </div>
            </div>
        </div>
    </section>
    
    <section class="challenge intro-section-padding" id="Rules">
        <div class="container">
            <div class="row wow fadeInUp">
                <div class="col-md-12">
                    <h2 style="margin-bottom: 1em">Challenge Rules</h2>
                    <p class="intro-paragraph" style="text-align: justify;">
                        <b style="font-weight: 600">1.</b> Challenge participants can use any encoding model derived from any source and trained on any type of data. 
                        However, using recorded brain responses for Friends season 7 or the OOD movie stimuli is prohibited.
                    </p>
                    <p class="intro-paragraph" style="text-align: justify;">
                        <b style="font-weight: 600">2.</b> The winning models will be determined based on their performance in predicting fMRI responses 
                        for the OOD movie stimuli during the model selection phase.
                    </p>
                    <p class="intro-paragraph" style="text-align: justify;">
                        <b style="font-weight: 600">3.</b> Challenge participants can make an unlimited number of submissions during the model building phase, 
                        and a maximum of ten submissions during the model selection phase (the leaderboard of each phase is automatically updated after every submission). 
                        Each challenge participant can only compete using one account. Creating multiple accounts to increase the number of possible submissions will 
                        result in disqualification to the challenge.
                    </p>
                    <p class="intro-paragraph" style="text-align: justify;">
                        <b style="font-weight: 600">4.</b> To promote open science, challenge participants who wish to be considered for the winners 
                        selection will need to submit a short report (~4-8 pages) describing their encoding algorithm to a preprint server 
                        (e.g. <i>arXiv</i>, <i>bioRxiv</i>), and send the PDF or preprint link to the Organizers by filling out <a href="https://forms.gle/RbouWUqYrm9pcWMXA" class="a-2021" target="_blank">this form</a>. 
                        You must submit the challenge report by the <a href="index.html#dates_table" class="a-2021">challenge report submission deadline</a> to be considered for the evaluation
                        of the challenge outcome. Furthermore, while all reports are encouraged to link to their code (e.g. GitHub), the top-3 
                        performing teams are required to make their code openly available. Participants that do not make their approach open and 
                        transparent cannot be considered. Along with monetary prizes, the top-3 performing teams will be invited to present their 
                        encoding models during a talk at the <a href="https://2025.ccneuro.org/" class="a-2021" target="_blank">Cognitive Computational Neuroscience (CCN) conference</a> held in Amsterdam (Netherlands) in August 2025.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="challenge intro-section-padding" id="Citation">
        <div class="container">
            <div class="row wow fadeInUp">
                <div class="col-md-12">
                    <h2 style="margin-bottom: 1em">Papers</h2>
                    <p class="intro-paragraph" style="text-align: justify;">
                        <a class="a-2021" href="index.html#cite">Click here to read the papers and for guidance on citation</a> 
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="challenge intro-section-padding" id="dates_table">
        <div class="container">
            <div class="row wow fadeInUp">
                    <div class="col-md-8">
                        <h2 style="margin-bottom: 1em">Important Dates</h2>
                        <table class="tables-2021">
                            <tr class="boxkey-challenge-2021">
                                <td>Challenge model building phase:</td>
                                <td>January 6th, 2025
                                    to
                                    July 6th, 2025, at 00:00am (UTC-0)
                                    </td>
                            <tr class="boxkey-challenge-2021">
                                <td>Challenge model selection phase:</td>
                                <td>July 6th, 2025
                                    to
                                    July 13th, 2025, at 00:00am (UTC-0)
                                    </td>
                            <tr class="boxkey-challenge-2021">
                                <td>Challenge report/code submission deadline:</td>
                                <td>July 25th, 2025</td>
                            <tr class="boxkey-challenge-2021">
                                <td>Challenge results released::</td>
                                <td>August 5th, 2025</td>
                            <tr class="boxkey-challenge-2021">
                                <td><a href="https://2025.ccneuro.org/" class="a-2021">Sessions at CCN 2025</a>:</td>
                                <td>August 12th&ndash;13th, 2025</td>
                        </table>
                    </div>
                </div>
                <p class="intro-paragraph" style="text-align: justify;">
                    <b style="font-weight: 600">If you participate in the challenge, use <a href="https://forms.gle/RbouWUqYrm9pcWMXA" class="a-2021" target="_blank">this form</a> to submit the challenge report and code.</b>
                </p>
            </div>
        </div>
    </section>

    <section class="challenge intro-section-padding" id="Acknowledgements">
        <div class="container">
            <div class="row wow fadeInUp">
                <div class="col-md-12">
                    <h2 style="margin-bottom: 1em">Acknowledgements</h2>
                    <p class="intro-paragraph" style="text-align: justify;">
                        The Algonauts Project 2025 challenge is supported by German Research Council (DFG) grants (CI 241/1-3, CI 241/1-7, INST 272/297-1), 
                        the European Research Council (ERC) starting grant (ERC-StG-2018-803370), the German Research Foundation (DFG Research Unit FOR 5368 ARENA), 
                        Unifying Neuroscience and Artificial Intelligence - Québec (UNIQUE), and The Hessian Center for Artificial Intelligence. 
                        The Courtois project on neural modelling was made possible by a generous donation from the Courtois foundation, administered 
                        by the Fondation Institut Gériatrie Montréal at CIUSSS du Centre-Sud-de-l’île-de-Montréal and University of Montreal. 
                        The CNeuroMod data used in the Algonauts 2025 Challenge has been openly shared under a Creative Commons CC0 license by a 
                        subset of CNeuroMod participants through the Canadian Open Neuroscience Platform (CONP), funded by Brain Canada and based 
                        at McGill University, Canada.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="challenge intro-section-padding" style="background:none" id="sponsors">
        <div class="container-fluid">
            <h2 class="section-heading text-center">Sponsors</h2>
            <div class="row content-flex-wrap">
                <div class="col-xs-12 col-md-4 img-align">
                    <a href="https://mitibmwatsonailab.mit.edu/">
                        <img class="img-logo-alt" style="height:140px; width:auto"  src="img/logo_mitibm.png">
                    </a>
                </div>
                <div class="col-xs-12 col-md-4 img-align">
                    <a href="https://erc.europa.eu/homepage">
                        <img class="img-sponsor" src="img/logo_erc.png">
                    </a>
                </div>
                <div class="col-xs-12 col-md-4 img-align">
                    <a href="https://www.dfg.de/en">
                        <img class="img-sponsor" style="height:80px" src="img/logo_dfg.jpg">
                    </a>
                </div>
                <div class="col-xs-12 col-md-4 img-align">
                    <a href="https://hessian.ai/">
                        <img class="img-sponsor" style="height:80px" src="img/logo_hessian-ai.svg">
                    </a>
                </div>
                <div class="col-xs-12 col-md-4 img-align">
                    <a href="https://www.unique.quebec/">
                        <img class="img-sponsor" style="height:120px; width:auto" src="img/2024/unique_logo_logo.png">
                    </a>
                </div>                
                <div class="col-xs-12 col-md-4 img-align">
                    <a href="https://2025.ccneuro.org/">
                        <img class="img-sponsor" style="height:80px" src="img/2024/ccn_logo.png">
                    </a>
                </div>
            </div>
        </div>
    </section>

    <!-- FOOTER -->
    <section class="to-top ">
        <div class="container ">
            <div class="row ">
                <div class="to-top-wrap ">
                    <a href="#top" class="top"><i class="fa fa-angle-up "></i></a>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <div class="container ">
            <div class="row ">
                <div class="col-md-7 ">
                    <div class="footer-links ">
                        <p>
                            Copyright © The Algonauts Project | <a href="https://accessibility.mit.edu/">Accessibility</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script>
        window.jQuery || document.write('<script src="js/vendor/jquery-1.11.2.min.js"><\/script>')
    </script>
    <script src="js/jquery.fancybox.pack.js"></script>
    <script src="js/vendor/bootstrap.min.js"></script>
    <script src="js/vendor/wow.min.js"></script>
    <script src="js/scripts.js"></script>
    <script src="js/jquery.flexslider-min.js"></script>
    <script src="bower_components/classie/classie.js"></script>
    <script src="bower_components/jquery-waypoints/lib/jquery.waypoints.min.js"></script>
    <script src="node_modules/angular/angular.min.js"></script>
    <script src="node_modules/angulargrid/angulargrid.js"></script>
    <script src="node_modules/angular-aria/angular-aria.min.js"></script>
    <script src="node_modules/angular-material/angular-material.min.js"></script>
    <script src="node_modules/angular-animate/angular-animate.min.js"></script>
    <script src="node_modules/angular-messages/angular-messages.min.js"></script>
    <script src="js/ng/src/app.js"></script>
    <script src="js/ng/src/controller.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const navToggle = document.querySelector('.nav-toggle');
            const navOverlay = document.querySelector('.nav-overlay');
            const navLinks = document.querySelectorAll('.overlay-nav ul li a');

            // Toggle the overlay when hamburger menu is clicked
            navToggle.addEventListener('click', function (event) {
                event.preventDefault();
                navOverlay.classList.toggle('open');
                navToggle.classList.toggle('active');
            });

            // Close the overlay when a navigation link is clicked
            navLinks.forEach(function (link) {
                link.addEventListener('click', function () {
                    navOverlay.classList.remove('open');
                    navToggle.classList.remove('active');
                });
            });
        });
    </script>
    <script>
        // Scroll event listener to toggle 'nav-scrolled' class
        // Add 'nav-scrolled' when the user scrolls beyond the hero section
        window.addEventListener('scroll', function() {
            const header = document.querySelector('header');
            const heroSection = document.querySelector('.hero-video-container');
            const heroHeight = heroSection.offsetHeight;

            if (window.scrollY > heroHeight - 100) {
                header.classList.add('nav-scrolled');
            } else {
                header.classList.remove('nav-scrolled');
            }
        });

    </script>
    <script> function externalLinks() { for (var c = document.getElementsByTagName("a"), a = 0; a < c.length; a++) { var b = c[a]; b.getAttribute("href") && b.hostname !== location.hostname && (b.target = "_blank") } }; externalLinks();</script>
</body>

</html>
